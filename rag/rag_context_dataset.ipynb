{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch torchvision torchaudio transformers datasets accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessiodesogus/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# First Part\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import jsonlines\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Second Part\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain.vectorstores import FAISS\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # This will be helpful when visualizing retriever outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/mnlpredators-project/' # CHANGE THIS TO YOUR PATH - [NICOLAS]\n",
    "full_preference_pairs_path = path + 'data/full_preference_pairs.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preference Pairs Dataset - Questions Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preference_pairs = pd.read_json(full_preference_pairs_path, orient='records', lines=False)\n",
    "print(full_preference_pairs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of questions:', len(full_preference_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 3 full questions\n",
    "# print(full_preference_pairs[['question_id','course_id','question_complete']].iloc[10])\n",
    "# print(full_preference_pairs[['question_id','course_id','question_complete']].iloc[20])\n",
    "# print(full_preference_pairs[['question_id','course_id','question_complete']].iloc[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Reranker and Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessiodesogus/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 22, 14:53:08] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessiodesogus/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reranker model\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Embedding model\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessiodesogus/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    # model_kwargs={\"device\": \"cpu\"},\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the Embedding Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\"faiss_index_8_keywords\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Most Relevant Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_document(\n",
    "    question: str,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 15,\n",
    "    num_docs_final: int = 5,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    # Gather documents with retriever\n",
    "    # print(\"=> Retrieving documents...\")\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        # print(\"=> Reranking documents...\")\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    # relevant_docs = relevant_docs[:num_docs_final]\n",
    "    \n",
    "    # Randomly sample num_docs_final documents\n",
    "    if len(relevant_docs) > num_docs_final:\n",
    "        relevant_docs = random.sample(relevant_docs, num_docs_final)\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"\\nDocument {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    return relevant_docs, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Relevance of the Document on a Simple Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a good distance metric to be used when you want to compute the similarity between documents independent of their length?\"\n",
    "relevant_docs, context = get_most_relevant_document(question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
    "print(\"The context is:\", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of an Answer with GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_wrapper.api_base = \"http://mnlp-backend-938795011.eu-central-1.elb.amazonaws.com\"\n",
    "gpt_wrapper.api_key = \"96bb5d54-0e6e-4614-ab1d-2e4263f6d20e\"\n",
    "\n",
    "model_args={\"temperature\": 0.7, \"top_p\": 0.7, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0, \"max_new_tokens\": 1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_prompt(question, context):\n",
    "    prompt = f'''Answer the following question: \"{question}\".\n",
    "        Use the following context if you deem necessary: \"{context}\". \n",
    "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
    "        Think step by step and explain your reasoning'''   \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_zero_shot(questions, model_args\n",
    "):\n",
    "    predictions = []\n",
    "    instruction=\"You are a helpful educational AI bot that answers questions for a student. Keep your response truthful and concise\"\n",
    "    with jsonlines.open(f\"data_wikipedia/rag_dataset_gpt3.5.jsonl\", mode=\"w\") as writer:\n",
    "\n",
    "        for question_dict in tqdm(questions):\n",
    "            question = question_dict['question_complete']  # Extract question text\n",
    "            \n",
    "            chat_id = random.randrange(0, 2**16,)\n",
    "            chat = Chat.create(name=f\"{chat_id}\")\n",
    "            \n",
    "            # _, context = get_most_relevant_document(question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
    "            _, context = get_most_relevant_document(question, KNOWLEDGE_VECTOR_DATABASE, reranker=None) # No reranker to have different documents\n",
    "            # print(\"The context is:\", context)\n",
    "            prompt = initial_prompt(question, context)\n",
    "            print(\"The final prompt is:\\n\", prompt)\n",
    "            \n",
    "            message = chat.ask(prompt, instruction=instruction, model_args=model_args)\n",
    "\n",
    "            preds = message.content.strip()\n",
    "            if preds:\n",
    "                pred = preds\n",
    "            else:\n",
    "                pred = \"none\"\n",
    "\n",
    "            print(\"Predicted answer:\", preds)\n",
    "            predictions.append(pred)\n",
    "\n",
    "            writer.write({\"course_id\": question_dict['course_id'], \"question_id\": question_dict['question_id'], \n",
    "                    \"question_body\": question, \"answer\": preds, \"chat_id\":chat_id})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Generation - Full Dataset of 1522 Questions (~2h30-3h00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1522 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final prompt is:\n",
      " Answer the following question: \"Question: Consider the following contains function defined on Iterable (in particular, it accepts both Vector and List).  def contains[A](l: Iterable[A], elem: A): Boolean =   val n = l.size   if n <= 5 then     for i <- l do       if i == elem then         return true     false   else     val (p0, p1) = parallel(       contains(l.take(n / 2), elem),       contains(l.drop(n / 2), elem)     )   p0 || p1 Let $n$$n$ be the size of l. Assume that drop and take run in $\\Theta(1)$ on Vector and $\\Theta(n)$ on List. What is the asymptotic depth of contains if it is called on a List?\".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "Since  does not contain the infinity at ∞, the construction can equally be applied to  taking  with horizontal slits removed to give a uniformizer . The uniformizer  now takes  to  with parallel slits removed at an angle of  to the -axis. In particular  =  leads to a uniformizer  for  with vertical slits removed. By uniqueness  = .\n",
      "\n",
      "Classification of simply connected Riemann surfaces \n",
      "\n",
      "Theorem. Any simply connected Riemann surface is conformally equivalent to either (1) the Riemann sphere (elliptic), (2) the complex plane (parabolic) or (3) the unit disk (hyperbolic).\n",
      "Document 1:::\n",
      "In mathematics, specifically in order theory and functional analysis, a sequence of positive elements  in a preordered vector space  (that is,  for all ) is called order summable if  exists in . \n",
      "For any , we say that a sequence  of positive elements of  is of type  if there exists some  and some sequence  in  such that  for all . \n",
      "\n",
      "The notion of order summable sequences is related to the completeness of the order topology.\n",
      "\n",
      "See also\n",
      "\n",
      "References\n",
      "\n",
      "Bibliography \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "Functional analysis\n",
      "Document 2:::\n",
      "One-in-three 3-SAT, together with its positive case, is listed as NP-complete problem \"LO4\" in the standard reference, Computers and Intractability: A Guide to the Theory of NP-Completeness\n",
      "by Michael R. Garey and David S. Johnson.  One-in-three 3-SAT was proved to be NP-complete by Thomas Jerome Schaefer as a special case of Schaefer's dichotomy theorem, which asserts that any problem generalizing Boolean satisfiability in a certain way is either in the class P or is NP-complete.\n",
      "\n",
      "Schaefer gives a construction allowing an easy polynomial-time reduction from 3-SAT to one-in-three 3-SAT.  Let \"(x or y or z)\" be a clause in a 3CNF formula.  Add six fresh boolean variables a, b, c, d, e, and f, to be used to simulate this clause and no other.\n",
      "\n",
      "Then the formula R(x,a,d) ∧ R(y,b,d) ∧ R(a,b,e) ∧ R(c,d,f) ∧ R(z,c,FALSE) is satisfiable by some setting of the fresh variables if and only if at least one of x, y, or z is TRUE, see picture (left).  Thus any 3-SAT instance with m clauses and n variables may be converted into an equisatisfiable one-in-three 3-SAT instance with 5m clauses and n+6m variables.\n",
      "Another reduction involves only four fresh variables and three clauses: R(¬x,a,b) ∧ R(b,y,c) ∧ R(c,d,¬z), see picture (right).\n",
      "\n",
      "Not-all-equal 3-satisfiability\n",
      "\n",
      "Another variant is the not-all-equal 3-satisfiability problem (also called NAE3SAT).\n",
      "Given a conjunctive normal form with three literals per clause, the problem is to determine if an assignment to the variables exists such that in no clause all three literals have the same truth value. This problem is NP-complete, too, even if no negation symbols are admitted, by Schaefer's dichotomy theorem.\n",
      "Document 3:::\n",
      "a series of relationships can be given in the form\n",
      "\n",
      "where An, Bn, Cn and Dn are positive integers. Plouffe gives a table of values:\n",
      "\n",
      "These integer constants may be expressed as sums over Bernoulli numbers, as given in (Vepstas, 2006) below.\n",
      " \n",
      "A fast algorithm for the calculation of Riemann's zeta function for any integer argument is given by E. A. Karatsuba.\n",
      "\n",
      "Negative integers\n",
      "In general, for negative integers (and also zero), one has\n",
      "\n",
      "The so-called \"trivial zeros\" occur at the negative even integers:\n",
      "\n",
      " (Ramanujan summation)\n",
      "\n",
      "The first few values for negative odd integers are\n",
      "\n",
      "However, just like the Bernoulli numbers, these do not stay small for increasingly negative odd values. For details on the first value, see 1 + 2 + 3 + 4 + · · ·.\n",
      "\n",
      "So ζ(m) can be used as the definition of all (including those for index 0 and 1) Bernoulli numbers.\n",
      "\n",
      "Derivatives\n",
      "The derivative of the zeta function at the negative even integers is given by\n",
      "\n",
      "The first few values of which are\n",
      "\n",
      "One also has\n",
      "\n",
      "where A is the Glaisher–Kinkelin constant.\n",
      "\n",
      "From the logarithmic derivative of the functional equation,\n",
      "\n",
      "Series involving ζ(n)\n",
      "The following sums can be derived from the generating function:\n",
      "\n",
      "where  is the digamma function.\n",
      "\n",
      "Series related to the Euler–Mascheroni constant (denoted by ) are\n",
      "\n",
      "and using the principal value \n",
      "\n",
      "which of course affects only the value at 1, these formulae can be stated as\n",
      "\n",
      "and show that they depend on the principal value of\n",
      "\n",
      "Nontrivial zeros \n",
      "\n",
      "Zeros of the Riemann zeta except negative even integers are called \"nontrivial zeros\". See Andrew Odlyzko's website for their tables and bibliographies.\n",
      "\n",
      "Ratios \n",
      "Although evaluating particular values of the zeta function is difficult, often certain ratios can be found by inserting particular values of the gamma function into the functional equation\n",
      "\n",
      "We have simple relations for half-integer arguments\n",
      "Document 4:::\n",
      "The AKS primality test (also known as Agrawal–Kayal–Saxena primality test and cyclotomic AKS test) is a deterministic primality-proving algorithm created and published by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in an article titled \"PRIMES is in P\". The algorithm was the first that can provably determine whether any given number is prime or composite in polynomial time, without relying on mathematical conjectures such as the generalized Riemann hypothesis. The proof is also notable for not relying on the field of analysis. In 2006 the authors received both the Gödel Prize and Fulkerson Prize for their work.\n",
      "\n",
      "Importance\n",
      "AKS is the first primality-proving algorithm to be simultaneously general, polynomial-time, deterministic, and unconditionally correct. Previous algorithms had been developed for centuries and achieved three of these properties at most, but not all four. \n",
      " The AKS algorithm can be used to verify the primality of any general number given. Many fast primality tests are known that work only for numbers with certain properties. For example, the Lucas–Lehmer test works only for Mersenne numbers, while Pépin's test can be applied to Fermat numbers only.\n",
      " The maximum running time of the algorithm can be bounded by a polynomial over the number of digits in the target number. ECPP and APR conclusively prove or disprove that a given number is prime, but are not known to have polynomial time bounds for all inputs.\n",
      " The algorithm is guaranteed to distinguish deterministically whether the target number is prime or composite. Randomized tests, such as Miller–Rabin and Baillie–PSW, can test any given number for primality in polynomial time, but are known to produce only a probabilistic result.\n",
      " The correctness of AKS is not conditional on any subsidiary unproven hypothesis. In contrast, Miller's version of the Miller–Rabin test is fully deterministic and runs in polynomial time over all inputs, but its correctness depends on the truth of the yet-unproven generalized Riemann hypothesis.\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1522 [00:06<2:35:34,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: The asymptotic depth of the contains function when called on a List is log(n), where n is the size of the List. This is because the function recursively splits the List in half until the size of the List is less than or equal to 5, resulting in a binary tree structure with a depth of log(n).\n",
      "The final prompt is:\n",
      " Answer the following question: \"Question: What is the asymptotic work of <code>parGroupyBy2</code>?\".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "In mathematics, specifically group theory, a descendant tree is a hierarchical structure that visualizes parent-descendant relations between isomorphism classes of finite groups of prime power order , for a fixed prime number  and varying integer exponents . Such groups are briefly called finite p-groups. The vertices of a descendant tree are isomorphism classes of finite p-groups.\n",
      "\n",
      "Additionally to their order , finite p-groups have two further related invariants, the nilpotency class  and the coclass . It turned out that descendant trees of a particular kind, the so-called pruned coclass trees whose infinitely many vertices share a common coclass , reveal a repeating finite pattern. These two crucial properties of finiteness and periodicity admit a characterization of all members of the tree by finitely many parametrized presentations. Consequently, descendant trees play a fundamental role in the classification of finite p-groups. By means of kernels and targets of Artin transfer homomorphisms, descendant trees can be endowed with additional structure.\n",
      "\n",
      "An important question is how the descendant tree  can actually be constructed for an assigned starting group which is taken as the root   of the tree. The p-group generation algorithm is a recursive process for constructing the descendant tree of a given finite p-group playing the role of the tree root. This algorithm is implemented in the computational algebra systems GAP and Magma.\n",
      "\n",
      "Definitions and terminology\n",
      "According to M. F. Newman, there exist several distinct definitions of the parent  of a finite p-group . The common principle is to form the quotient  of  by a suitable normal subgroup  which can be either\n",
      "\n",
      " the centre  of , whence  is called the central quotient of , or\n",
      " the last non-trivial term  of the lower central series of , where  denotes the nilpotency class of , or\n",
      " the last non-trivial term  of the lower exponent-p central series of , where  denotes the exponent-p class of , or\n",
      " the last non-trivial term  of the derived series of , where  denotes the derived length of .\n",
      "Document 1:::\n",
      "|-\n",
      "| align=left|  || 1 || 11 || || || || || || || || 12\n",
      "|-\n",
      "| align=left| c ||207 ||32 ||3 ||1 ||6 || || ||16 || 1 || 266\n",
      "|- valign=top\n",
      "| align=left| -j-a || rowspan=2| || rowspan=2| 1821 || rowspan=2| 114 || rowspan=2| || rowspan=2| || rowspan=2| 22 || rowspan=2| || rowspan=2| || rowspan=2| 43 || rowspan=2| 5 || rowspan=2| 2005\n",
      "|-\n",
      "| align=left| -ia\n",
      "|- valign=top\n",
      "| rowspan=3 align=left| -ė || align=left|  || 2668 || 2895 || 14 || 30 || 125 || 14 || 1 || 202 || 59 || 6008\n",
      "|-\n",
      "| align=left|  || || 4 || || || || || || || || 4\n",
      "|-\n",
      "| align=left| c || 2 || 6 || || || 7 || || || 4 || || 19\n",
      "|- valign=top\n",
      "| align=left rowspan=3| -is || align=left|  || 50 || 2 || 7 || 10 || 76 || 1 || || 99 || 10 || 255\n",
      "|-\n",
      "| align=left|  || 1 || 1 || 2 || 6 || 10 || || 1 || 3 || 3 || 27\n",
      "|-\n",
      "| align=left| c || || || || || 3 || || 1 || 1 || 3 || 8\n",
      "|-\n",
      "Document 2:::\n",
      "Choice of tilting parameter\n",
      "\n",
      "Siegmund's algorithm\n",
      "Assume i.i.d. X's with light tailed distribution and . In order to estimate   where , when   is large and hence   small, the algorithm uses exponential tilting to derive the importance distribution. The algorithm is used in many aspects, such as sequential tests, G/G/1 queue waiting times, and  is used as the probability of ultimate ruin in ruin theory. In this context, it is logical to ensure that . The criterion , where  is s.t.  achieves this. Siegmund's algorithm uses , if it exists, where  is defined in the following way: \n",
      ". \n",
      "It has been shown that  is the only tilting parameter producing bounded relative error ().\n",
      "\n",
      "Black-Box algorithms\n",
      "We can only see the input and output of a black box, without knowing its structure. The algorithm is to use only minimal information on its structure. When we generate random numbers,  the output may not be \n",
      "within the same common parametric class, such as normal or exponential distributions. An automated way may be used to perform ECM. Let be i.i.d. r.v.’s with distribution ; for simplicity we assume . Define , where , . . . are independent (0, 1) uniforms. A randomized stopping time for , . . . is then a stopping time w.r.t. the filtration , . . . Let further  be a class of distributions  on  with  and define  by . We define a black-box algorithm for ECM for the given  and the given class  of distributions as a pair of a randomized stopping time   and an  measurable r.v.  such that  is distributed according to  for any . Formally, we write this as  for all  . In other words, the rules of the game are that the algorithm may use\n",
      "simulated values from   and additional uniforms to produce an r.v. from  .\n",
      "\n",
      "Common distributions\n",
      "\n",
      "See also \n",
      " Importance sampling\n",
      " Rejection sampling\n",
      " Monte Carlo method\n",
      " Exponential family\n",
      " Esscher transform\n",
      "\n",
      "References\n",
      "\n",
      "Sampling techniques\n",
      "Document 3:::\n",
      "Java will not free the object until it has proven that the object is once again unreachable, but will not run the finalizer more than once.\n",
      "\n",
      "In Python, prior to Python 3.4, the standard CPython implementation would treat resurrected objects identically to other objects (which had never been finalized), making indestructible objects possible. Further, it would not garbage collect cycles that contained an object with a finalizer, to avoid possible problems with object resurrection. Starting in Python 3.4, behavior is largely the same as Java: objects are only finalized once (being marked as \"already finalized\"), garbage collection of cycles is in two phases, with the second phase checking for resurrected objects.\n",
      "\n",
      "Objective-C 2.0 will put resurrected objects into a \"zombie\" state, where they log all messages sent to them, but do nothing else. See also Automatic Reference Counting: Zeroing Weak References for handling of weak references.\n",
      "\n",
      "In the .NET Framework, notably C# and VB.NET, object finalization is determined by a finalization \"queue\", which is checked during object destruction.\n",
      "Objects with a finalizer are placed in this queue on creation, and dequeued when the finalizer is called, but can be manually dequeued (prior to finalization) with SuppressFinalize or re-enqueued with ReRegisterForFinalize. Thus by default objects with finalizers are finalized at most once, but this finalization can be suppressed, or objects can be finalized multiple times if they are resurrected (made accessible again) and then re-enqueued for finalization. Further, weak references by default do not track resurrection, meaning a weak reference is not updated if an object is resurrected; these are called short weak references, and weak references that track resurrection are called long weak references.\n",
      "Document 4:::\n",
      "Tail-recursive functions\n",
      "Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive.  In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes.  With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls, a tail-recursive function such as gcd will execute using constant space.  Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops.\n",
      "\n",
      "The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.\n",
      "\n",
      "Order of execution\n",
      "\n",
      "Consider these two functions:\n",
      "\n",
      "Function 1\n",
      "void recursiveFunction(int num) {\n",
      "    printf(\"%d\\n\", num);\n",
      "    if (num < 4)\n",
      "        recursiveFunction(num + 1);\n",
      "}\n",
      "\n",
      "Function 2\n",
      "void recursiveFunction(int num) {\n",
      "    if (num < 4)\n",
      "        recursiveFunction(num + 1);\n",
      "    printf(\"%d\\n\", num);\n",
      "}\n",
      "\n",
      "Function 2 is function 1 with the lines swapped.\n",
      "\n",
      "In the case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached. \n",
      "\n",
      "Also note that the order of the print statements is reversed, which is due to the way the functions and statements are stored on the call stack.\n",
      "\n",
      "Recursive procedures\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1522 [00:11<2:23:39,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: The asymptotic work of `parGroupBy2` is not provided in the extracted documents.\n",
      "The final prompt is:\n",
      " Answer the following question: \"Question: We have a collection of rectangles in a plane, whose sides are aligned with the coordinate axes. Each rectangle is represented by its lower left corner $(x_1,y_1)$ and its upper right corner $(x_2,y_2)$. All coordinates are of type Long. We require $x_1 \\le x_2$ and $y_1 \\le y_2$. Define a case class Rectangle storing two corners. \".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "To describe a finite projective plane of order N(≥ 2) using non-homogeneous coordinates and a planar ternary ring:\n",
      "Let one point be labelled (∞).\n",
      "Label N points, (r) where r = 0, ..., (N − 1).\n",
      "Label N2 points, (r, c) where r, c = 0, ..., (N − 1).\n",
      "On these points, construct the following lines:\n",
      "One line [∞] = { (∞), (0), ..., (N − 1)}\n",
      "N lines [c] = {(∞), (c,0), ..., (c, N − 1)}, where c = 0, ..., (N − 1)\n",
      "N2 lines [r, c] = {(r) and the points (x, T(x,r,c)) }, where x, r, c = 0, ..., (N − 1) and T is the ternary operator of the planar ternary ring.\n",
      "Document 1:::\n",
      "SageMath (previously Sage or SAGE, \"System for Algebra and Geometry Experimentation\") is a computer algebra system (CAS) with features covering many aspects of mathematics, including algebra, combinatorics, graph theory, numerical analysis, number theory, calculus and statistics.\n",
      "\n",
      "The first version of SageMath was released on 24 February 2005 as free and open-source software under the terms of the GNU General Public License version 2, with the initial goals of creating an \"open source alternative to Magma, Maple, Mathematica, and MATLAB\". The originator and leader of the SageMath project, William Stein, was a mathematician at the University of Washington.\n",
      "\n",
      "SageMath uses a syntax resembling Python's, supporting procedural, functional and object-oriented constructs.\n",
      "\n",
      "Development\n",
      "\n",
      "Stein realized when designing Sage that there were many open-source mathematics software packages already written in different languages, namely C, C++, Common Lisp, Fortran and Python.\n",
      "\n",
      "Rather than reinventing the wheel, Sage (which is written mostly in Python and Cython) integrates many specialized CAS software packages into a common interface, for which a user needs to know only Python. However, Sage contains hundreds of thousands of unique lines of code adding new functions and creating the interfaces among its components.\n",
      "\n",
      "SageMath uses both students and professionals for development. The development of SageMath is supported by both volunteer work and grants. However, it was not until 2016 that the first full-time Sage developer was hired (funded by an EU grant). The same year, Stein described his disappointment with a lack of academic funding and credentials for software development, citing it as the reason for his decision to leave his tenured academic position to work full-time on the project in a newly founded company, SageMath, Inc.\n",
      "\n",
      "Achievements\n",
      " 2007: first prize in the scientific software division of Les Trophées du Libre, an international competition for free software.\n",
      " 2012: one of the projects selected for the Google Summer of Code.\n",
      " 2013: ACM/SIGSAM Jenks Prize.\n",
      "Document 2:::\n",
      "One may then ask, \"what else behaves like this?\" Well, obviously the 3D rotation matrices do; after all, the whole point is that they do correctly, perfectly mathematically describe rotations in 3D space. As it happens, though, there are also 2x2, 4x4, 5x5, ... matrices that also have this property. One may reasonably ask \"OK, so what is the shape of their manifolds?\". For the 2x2 case, the Lie algebra is called su(2) and the manifold is called SU(2), and quite curiously, the manifold of SU(2) is the 3-sphere (but without the projective identification of polar opposites). \n",
      "\n",
      "This now allows one to play a bit of a trick. Take a vector  in ordinary 3D space (our physical space) and apply a rotation matrix  to it. One obtains a rotated vector . This is the result of applying an ordinary, \"common sense\" rotation to . But one also has the Pauli matrices ; these are 2x2 complex matrices that have the Lie algebra property that  and so these model the  behavior of infinitesimal rotations. Consider then the product . The \"double covering\" is the property that there exists not one, but two 2x2 matrices  such that\n",
      "\n",
      "Here,  denotes the inverse of ; that is,   The matrix  is an element of SU(2), and so for every matrix  in SO(3), there are two corresponding : both  and  will do the trick. These two are the polar-opposites, and the projection is just boils down to the trivial observation that  The tangeloid game is meant to illustrate that a 360 degree rotation takes one on a path from   to .  This is quite precise: one can consider a sequence of small rotations  and the corresponding movement of ; the result does change sign. In terms of rotation angles  the  matrix will have a  in it, but the matching  will have a  in it.  Further elucidation requires actually writing out these formulas.\n",
      "Document 3:::\n",
      "Examples \n",
      "An easy class of examples can be cooked up by considering crossed modules, or equivalently the data of a morphism of groupswhich has an equivalent description as the groupoid internal to the category of groupswhereare the structure morphisms for this groupoid. Since groups embed in the category of groupoids sending a group  to the category  with a single object and morphisms giving the group , the structure above gives a double groupoid. Let's give an explicit example: from the group extensionand the embedding of , there is an associated double groupoid from the two term complex of groupswith kernel is  and cokernel is given by . This gives an associated homotopy type  with and Its postnikov invariant can be determined by the class of  in the group cohomology group . Because this is not a trivial crossed-module, it's postnikov invariant is , giving a homotopy type which is not equivalent to the geometric realization of a simplicial abelian group.\n",
      "\n",
      "Homotopy double groupoid\n",
      "A generalisation to dimension 2 of the fundamental groupoid on a set of base was given by Brown and Higgins in 1978 as follows. Let  be a triple of spaces, i.e. . Define  to be the set of homotopy classes rel vertices of maps of a square into X which take the edges into A and the vertices into C. It is not entirely trivial to prove that the natural compositions of such squares in two directions are inherited by these homotopy classes to give a double groupoid, which also has an extra structure of so-called connections necessary to discuss the idea of commutative cube in a double groupoid.  This double groupoid is used in an essential way to prove a two-dimensional Seifert-van Kampen theorem, which gives new information and computations on second relative homotopy groups as part of a crossed module. For more information, see Part I of the book by Brown, Higgins, Sivera listed below.\n",
      "\n",
      "Convolution algebra\n",
      "A convolution C*-algebra of a double groupoid can also be constructed by employing the square diagram D of a double groupoid.\n",
      "\n",
      " Double groupoid category \n",
      "The category whose objects are double groupoids and whose morphisms are double groupoid homomorphisms that are double groupoid diagram (D) functors is called the double groupoid category, or the category of double groupoids.\n",
      "Document 4:::\n",
      "Since  does not contain the infinity at ∞, the construction can equally be applied to  taking  with horizontal slits removed to give a uniformizer . The uniformizer  now takes  to  with parallel slits removed at an angle of  to the -axis. In particular  =  leads to a uniformizer  for  with vertical slits removed. By uniqueness  = .\n",
      "\n",
      "Classification of simply connected Riemann surfaces \n",
      "\n",
      "Theorem. Any simply connected Riemann surface is conformally equivalent to either (1) the Riemann sphere (elliptic), (2) the complex plane (parabolic) or (3) the unit disk (hyperbolic).\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1522 [00:19<2:53:31,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: The question asks to define a case class Rectangle storing two corners $(x_1,y_1)$ and $(x_2,y_2)$. The case class Rectangle can be defined in Scala as follows:\n",
      "\n",
      "```scala\n",
      "case class Rectangle(x1: Long, y1: Long, x2: Long, y2: Long)\n",
      "```\n",
      "\n",
      "This case class represents a rectangle with its lower left corner at coordinates $(x_1, y_1)$ and its upper right corner at coordinates $(x_2, y_2)$.\n",
      "The final prompt is:\n",
      " Answer the following question: \"Question: Which of the following scheduler policies are preemptive?\n",
      "\n",
      "Options:\n",
      "A. FIFO (First In, First Out)\n",
      "B. SJF (Shortest Job First)\n",
      "C. STCF (Shortest Time to Completion First)\n",
      "D. RR (Round Robin)\".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "A* uses this heuristic to improve on the behavior relative to Dijkstra's algorithm. When the heuristic evaluates to zero, A* is equivalent to Dijkstra's algorithm. As the heuristic estimate increases and gets closer to the true distance, A* continues to find optimal paths, but runs faster (by virtue of examining fewer nodes). When the value of the heuristic is exactly the true distance, A* examines the fewest nodes. (However, it is generally impractical to write a heuristic function that always computes the true distance, as the same comparison result can often be reached using simpler calculations – for example, using Manhattan distance over Euclidean distance in two-dimensional space.) As the value of the heuristic increases, A* examines fewer nodes but no longer guarantees an optimal path. In many applications (such as video games) this is acceptable and even desirable, in order to keep the algorithm running quickly.\n",
      "\n",
      "Sample algorithm \n",
      "\n",
      "This is a fairly simple and easy-to-understand pathfinding algorithm for tile-based maps. To start off, you have a map, a start coordinate and a destination coordinate. The map will look like this, X being walls, S being the start, O being the finish and _ being open spaces, the numbers along the top and right edges are the column and row numbers:\n",
      "\n",
      "   1 2 3 4 5 6 7 8\n",
      " X X X X X X X X X X\n",
      " X _ _ _ X X _ X _ X 1\n",
      " X _ X _ _ X _ _ _ X 2\n",
      " X S X X _ _ _ X _ X 3\n",
      " X _ X _ _ X _ _ _ X 4\n",
      " X _ _ _ X X _ X _ X 5\n",
      " X _ X _ _ X _ X _ X 6\n",
      " X _ X X _ _ _ X _ X 7\n",
      " X _ _ O _ X _ _ _ X 8\n",
      " X X X X X X X X X X\n",
      "\n",
      "First, create a list of coordinates, which we will use as a queue. The queue will be initialized with one coordinate, the end coordinate. Each coordinate will also have a counter variable attached (the purpose of this will soon become evident). Thus, the queue starts off as ((3,8,0)).\n",
      "Document 1:::\n",
      "The National Health Service (Scotland) (Injury Benefits) Regulations 1998 (S.I. 1998 No. 1594 (S. 84)])\n",
      " The New Opportunities Fund (Specification of Initiatives) Order 1998 (S.I. 1998 No. 1598)\n",
      " The Football Spectators (Seating) Order 1998 (S.I. 1998 No. 1599)\n",
      " The National Health Service (General Medical Services) (Scotland) Amendment (No.3) Regulations 1998 (S.I. 1998 No. 1600 (S.85))\n",
      "Document 2:::\n",
      "Choice of tilting parameter\n",
      "\n",
      "Siegmund's algorithm\n",
      "Assume i.i.d. X's with light tailed distribution and . In order to estimate   where , when   is large and hence   small, the algorithm uses exponential tilting to derive the importance distribution. The algorithm is used in many aspects, such as sequential tests, G/G/1 queue waiting times, and  is used as the probability of ultimate ruin in ruin theory. In this context, it is logical to ensure that . The criterion , where  is s.t.  achieves this. Siegmund's algorithm uses , if it exists, where  is defined in the following way: \n",
      ". \n",
      "It has been shown that  is the only tilting parameter producing bounded relative error ().\n",
      "\n",
      "Black-Box algorithms\n",
      "We can only see the input and output of a black box, without knowing its structure. The algorithm is to use only minimal information on its structure. When we generate random numbers,  the output may not be \n",
      "within the same common parametric class, such as normal or exponential distributions. An automated way may be used to perform ECM. Let be i.i.d. r.v.’s with distribution ; for simplicity we assume . Define , where , . . . are independent (0, 1) uniforms. A randomized stopping time for , . . . is then a stopping time w.r.t. the filtration , . . . Let further  be a class of distributions  on  with  and define  by . We define a black-box algorithm for ECM for the given  and the given class  of distributions as a pair of a randomized stopping time   and an  measurable r.v.  such that  is distributed according to  for any . Formally, we write this as  for all  . In other words, the rules of the game are that the algorithm may use\n",
      "simulated values from   and additional uniforms to produce an r.v. from  .\n",
      "\n",
      "Common distributions\n",
      "\n",
      "See also \n",
      " Importance sampling\n",
      " Rejection sampling\n",
      " Monte Carlo method\n",
      " Exponential family\n",
      " Esscher transform\n",
      "\n",
      "References\n",
      "\n",
      "Sampling techniques\n",
      "Document 3:::\n",
      "Applications\n",
      "The basic exertion-oriented platform was developed at GE Global Research Center with the partners of the FIPER project (1999-2003). FIPER was used at that time to design aircraft engines. The Multidisciplinary Science and Technology Center, the United States Air Force Research Laboratory/WPAFB is using SORCER to address the physics-based distributed collaborative design for aerospace vehicle development. SORCER was selected for comparative study of evolutionary computing of optimization techniques at the Cranfield University, UK. In China, SORCER is used as noise mapping platform for urban traffic, a resource integration platform, engineering collaborative design and manufacturing environment, and at the Wright State University as a collaborative computational framework for multidisciplinary and reliability-based analysis and optimization.\n",
      "\n",
      "History \n",
      "SORCER follows up on the FIPER project (1999-2003) - funded by National Institute of Standards and Technology Advanced Technology Program. The FIPER software environment was developed and demonstrated at the GE Global Research Center (Chief software architect and lead developer Michael Sobolewski  and engineering application development led by R. Kolonay) in collaboration with GE Aviation (Cincinnati, OH), Goodrich Corporation Aerostructures Group (Chula Vista, CA), Parker Hannifin Corporation (Mentor, OH), Engineous Software, Inc. (Cary, NC) and Ohio University (Athens, OH). When the project was finished M. Sobolewski established the SORCER Laboratory at Texas Tech University (2002-2009) where he continued his FIPER-based research. The SORCER Laboratory was partially funded by General Electric, Texas Tech University, Sun Microsystems, Air Force Research Laboratory, and others. During that time 28 graduate research studies (M.S. and Ph.D.) were completed all of which contributed to the development of the SORCER platform and the foundations of federated service-oriented computing. In the meantime, a number of collaborative SORCER-based projects (2007-2010) were realized together with universities from other countries (Beijing Jiaotong University, China; Beihang University, China; Ulyanovsk State University and Samara State Aerospace University, Russia).\n",
      "Document 4:::\n",
      "One-in-three 3-SAT, together with its positive case, is listed as NP-complete problem \"LO4\" in the standard reference, Computers and Intractability: A Guide to the Theory of NP-Completeness\n",
      "by Michael R. Garey and David S. Johnson.  One-in-three 3-SAT was proved to be NP-complete by Thomas Jerome Schaefer as a special case of Schaefer's dichotomy theorem, which asserts that any problem generalizing Boolean satisfiability in a certain way is either in the class P or is NP-complete.\n",
      "\n",
      "Schaefer gives a construction allowing an easy polynomial-time reduction from 3-SAT to one-in-three 3-SAT.  Let \"(x or y or z)\" be a clause in a 3CNF formula.  Add six fresh boolean variables a, b, c, d, e, and f, to be used to simulate this clause and no other.\n",
      "\n",
      "Then the formula R(x,a,d) ∧ R(y,b,d) ∧ R(a,b,e) ∧ R(c,d,f) ∧ R(z,c,FALSE) is satisfiable by some setting of the fresh variables if and only if at least one of x, y, or z is TRUE, see picture (left).  Thus any 3-SAT instance with m clauses and n variables may be converted into an equisatisfiable one-in-three 3-SAT instance with 5m clauses and n+6m variables.\n",
      "Another reduction involves only four fresh variables and three clauses: R(¬x,a,b) ∧ R(b,y,c) ∧ R(c,d,¬z), see picture (right).\n",
      "\n",
      "Not-all-equal 3-satisfiability\n",
      "\n",
      "Another variant is the not-all-equal 3-satisfiability problem (also called NAE3SAT).\n",
      "Given a conjunctive normal form with three literals per clause, the problem is to determine if an assignment to the variables exists such that in no clause all three literals have the same truth value. This problem is NP-complete, too, even if no negation symbols are admitted, by Schaefer's dichotomy theorem.\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1522 [00:26<2:47:38,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: The preemptive scheduler policies among the options provided are:\n",
      "C. STCF (Shortest Time to Completion First)\n",
      "D. RR (Round Robin)\n",
      "The final prompt is:\n",
      " Answer the following question: \"Question: In this week's lecture, you have been introduced to the aggregate method of ParSeq[A] (and other parallel data structures). It has the following signature:  def aggregate[B](z: B)(f: (B, A) => B, g: (B, B) => B): B Discuss, as a group, what aggregate does and what its arguments represent. Consider the parallel sequence xs containing the three elements x1, x2 and x3. Also consider the following call to aggregate:  xs.aggregate(z)(f, g) The above call might potentially result in the following computation:  f(f(f(z, x1), x2), x3) But it might also result in other computations. Come up with at least two other computations in terms of f and g that may result from the above call to aggregate.  Below are other examples of calls to aggregate. In each case, check if the call can lead to different results depending on the strategy used by aggregate to aggregate all values contained in data down to a single value. You should assume that data is a parallel sequence of values of type BigInt. 1. data.aggregate(1)(_ + _, _ + _)\".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "Most instructions have dot-letter suffixes, permitting operations to occur on 8-bit bytes (\".b\"), 16-bit words (\".w\"), and 32-bit longs (\".l\").\n",
      "\n",
      "Like many CPUs of its era the cycle timing of some instructions varied depending on the source operand(s). For example, the unsigned multiply instruction takes (38+2n) clock cycles to complete where 'n' is equal to the number of bits set in the operand. To create a function that took a fixed cycle count required the addition of extra code after the multiply instruction. This would typically consume extra cycles for each bit that wasn't set in the original multiplication operand.\n",
      "\n",
      "Most instructions are dyadic, that is, the operation has a source, and a destination, and the destination is changed. Notable instructions were:\n",
      "Document 1:::\n",
      "One-in-three 3-SAT, together with its positive case, is listed as NP-complete problem \"LO4\" in the standard reference, Computers and Intractability: A Guide to the Theory of NP-Completeness\n",
      "by Michael R. Garey and David S. Johnson.  One-in-three 3-SAT was proved to be NP-complete by Thomas Jerome Schaefer as a special case of Schaefer's dichotomy theorem, which asserts that any problem generalizing Boolean satisfiability in a certain way is either in the class P or is NP-complete.\n",
      "\n",
      "Schaefer gives a construction allowing an easy polynomial-time reduction from 3-SAT to one-in-three 3-SAT.  Let \"(x or y or z)\" be a clause in a 3CNF formula.  Add six fresh boolean variables a, b, c, d, e, and f, to be used to simulate this clause and no other.\n",
      "\n",
      "Then the formula R(x,a,d) ∧ R(y,b,d) ∧ R(a,b,e) ∧ R(c,d,f) ∧ R(z,c,FALSE) is satisfiable by some setting of the fresh variables if and only if at least one of x, y, or z is TRUE, see picture (left).  Thus any 3-SAT instance with m clauses and n variables may be converted into an equisatisfiable one-in-three 3-SAT instance with 5m clauses and n+6m variables.\n",
      "Another reduction involves only four fresh variables and three clauses: R(¬x,a,b) ∧ R(b,y,c) ∧ R(c,d,¬z), see picture (right).\n",
      "\n",
      "Not-all-equal 3-satisfiability\n",
      "\n",
      "Another variant is the not-all-equal 3-satisfiability problem (also called NAE3SAT).\n",
      "Given a conjunctive normal form with three literals per clause, the problem is to determine if an assignment to the variables exists such that in no clause all three literals have the same truth value. This problem is NP-complete, too, even if no negation symbols are admitted, by Schaefer's dichotomy theorem.\n",
      "Document 2:::\n",
      "Other studies might define an inverse function of one where m is set to a constant, such that the inverse applies to a particular row. \n",
      "\n",
      "The inverse of the Ackermann function is primitive recursive.\n",
      "\n",
      "Use as benchmark\n",
      "The Ackermann function, due to its definition in terms of extremely deep recursion, can be used as a benchmark of a compiler's ability to optimize recursion. The first published use of Ackermann's function in this way was in 1970 by Dragoș Vaida and, almost simultaneously, in 1971, by Yngve Sundblad.\n",
      "\n",
      "Sundblad's seminal paper was taken up by Brian Wichmann (co-author of the Whetstone benchmark) in a trilogy of papers written between 1975 and 1982.\n",
      "\n",
      "See also\n",
      "\n",
      " Computability theory\n",
      " Double recursion\n",
      " Fast-growing hierarchy\n",
      " Goodstein function\n",
      " Primitive recursive function\n",
      " Recursion (computer science)\n",
      "\n",
      "Notes\n",
      "\n",
      "References\n",
      "\n",
      "Bibliography\n",
      "\n",
      "External links\n",
      " \n",
      " \n",
      " \n",
      " An animated Ackermann function calculator\n",
      " Ackerman function implemented using a for loop\n",
      " Scott Aaronson, Who can name the biggest number? (1999)\n",
      " Ackermann functions. Includes a table of some values.\n",
      " Hyper-operations: Ackermann's Function and New Arithmetical Operation\n",
      " Robert Munafo's Large Numbers describes several variations on the definition of A.\n",
      " Gabriel Nivasch, Inverse Ackermann without pain on the inverse Ackermann function.\n",
      " Raimund Seidel, Understanding the inverse Ackermann function (PDF presentation).\n",
      " The Ackermann function written in different programming languages, (on Rosetta Code)\n",
      " Ackermann's Function (Archived 2009-10-24)—Some study and programming by Harry J. Smith.\n",
      "\n",
      "Arithmetic\n",
      "Large integers\n",
      "Special functions\n",
      "Theory of computation\n",
      "Computability theory\n",
      "Document 3:::\n",
      "In computer science, array programming refers to solutions which allow the application of operations to an entire set of values at once. Such solutions are commonly used in scientific and engineering settings. \n",
      "\n",
      "Modern programming languages that support array programming (also known as vector or multidimensional languages) have been engineered specifically to  generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays. These include APL, J, Fortran 90, MATLAB, Analytica, TK Solver (as lists), Octave, R, Cilk Plus, Julia, Perl Data Language (PDL). In these languages, an operation that operates on entire arrays can be called a vectorized operation, regardless of whether it is executed on a vector processor, which implements vector instructions. Array programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon to find array programming language one-liners that require several pages of object-oriented code.\n",
      "\n",
      "Concepts of array\n",
      "The fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.\n",
      "\n",
      "Kenneth E. Iverson described the rationale behind array programming (actually referring to APL) as follows:\n",
      "\n",
      "The basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.\n",
      "\n",
      "Function rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.\n",
      "Document 4:::\n",
      "The AKS primality test (also known as Agrawal–Kayal–Saxena primality test and cyclotomic AKS test) is a deterministic primality-proving algorithm created and published by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in an article titled \"PRIMES is in P\". The algorithm was the first that can provably determine whether any given number is prime or composite in polynomial time, without relying on mathematical conjectures such as the generalized Riemann hypothesis. The proof is also notable for not relying on the field of analysis. In 2006 the authors received both the Gödel Prize and Fulkerson Prize for their work.\n",
      "\n",
      "Importance\n",
      "AKS is the first primality-proving algorithm to be simultaneously general, polynomial-time, deterministic, and unconditionally correct. Previous algorithms had been developed for centuries and achieved three of these properties at most, but not all four. \n",
      " The AKS algorithm can be used to verify the primality of any general number given. Many fast primality tests are known that work only for numbers with certain properties. For example, the Lucas–Lehmer test works only for Mersenne numbers, while Pépin's test can be applied to Fermat numbers only.\n",
      " The maximum running time of the algorithm can be bounded by a polynomial over the number of digits in the target number. ECPP and APR conclusively prove or disprove that a given number is prime, but are not known to have polynomial time bounds for all inputs.\n",
      " The algorithm is guaranteed to distinguish deterministically whether the target number is prime or composite. Randomized tests, such as Miller–Rabin and Baillie–PSW, can test any given number for primality in polynomial time, but are known to produce only a probabilistic result.\n",
      " The correctness of AKS is not conditional on any subsidiary unproven hypothesis. In contrast, Miller's version of the Miller–Rabin test is fully deterministic and runs in polynomial time over all inputs, but its correctness depends on the truth of the yet-unproven generalized Riemann hypothesis.\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1522 [00:31<2:41:08,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: I'm sorry, but the provided question is related to computer science and requires a detailed understanding of parallel data structures and computations. If you have any specific queries or need clarification on any concept, feel free to ask.\n",
      "The final prompt is:\n",
      " Answer the following question: \"Question: Suppose we run JOS and set a breakpoint at syscall (in lib/syscall.c). What are the Current Privilege Level (CPL) before invoking the syscall function and after executing the int 0x30 instruction?\n",
      "\n",
      "Options:\n",
      "A. 0 3\n",
      "B. 0 0\n",
      "C. 3 0\n",
      "D. 3 3\".\n",
      "        Use the following context if you deem necessary: \"\n",
      "Extracted documents:\n",
      "\n",
      "Document 0:::\n",
      "The German standard DIN 66303 is a character set standard, which is used for character encoding in computer systems. The standard DIN 66303 bears the title \"Information Technology: 8-Bit-Code\" and was established in November 1986 (DIN 66303:1986-11). The most recent edition is from June 2000 (DIN 66303:2000-06).\n",
      "\n",
      "The character set of the 2000 edition (DIN 66303:2000-06) corresponds in layout and repertoire to the international standard ISO/IEC 8859-1. The still often-used forerunner DIN 66303:1986-11 specified two code pages, the General Reference Version of the 8-Bit-Code (, ARV8) and the German Reference Version of the 8-Bit-Code (, DRV8).\n",
      "\n",
      "DRV8 is an extension of DIN 66003 (the German adaptation of ISO/IEC 646) with European characters, whereas ARV8 represents a re-arrangement of the DIN 66003 characters to their internationally used (ISO-8859-1 or DEC MCS) code points.\n",
      "\n",
      "Tables for the 1986 edition\n",
      "\n",
      "DIN 66303:1986-11 – German Reference Version of the 8-bit Code (DRV8) \n",
      "\n",
      "The DRV8 code corresponds to ISO-8859-1 with certain characters swapped, such as to make it an extension of DIN 66003 as opposed to of ASCII.\n",
      "\n",
      "DIN 66303:1986-11 – General Reference Version of the 8-Bit-Code (ARV8) \n",
      "\n",
      "The name \"ARV8\" is associated with ISO-8859-1 without rearrangement. Shown below is the common subset of the Latin parts of ISO 8859, which corresponds to the definition of ARV8 in the 1986 edition of DIN 66303.\n",
      "\n",
      "References\n",
      "\n",
      "Character encoding\n",
      "ISO/IEC 8859\n",
      "DIN standards\n",
      "Document 1:::\n",
      "HelenOS supports PATA, SATA, USB mass storage, USB HID, an Atheros USB WiFi dongle, several Ethernet network cards, SoundBlaster 16 and Intel HDA audio devices, serial ports, keyboards, mice and framebuffers.\n",
      "\n",
      "Research and academic use \n",
      "HelenOS is being used for research in the area of software components and verification by the Department of Distributed and Dependable Systems, Charles University, Prague. Besides that, HelenOS has been used by students as a platform for software projects and master theses.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      " HelenOS home page\n",
      " HelenOS theses, papers and documentation.\n",
      " HelenOS on GitHub\n",
      "\n",
      "Free software operating systems\n",
      "Microkernel-based operating systems\n",
      "MIPS operating systems\n",
      "X86 operating systems\n",
      "Microkernels\n",
      "Software using the BSD license\n",
      "Hobbyist operating systems\n",
      "Document 2:::\n",
      "In computer programming, the term hooking covers a range of techniques used to alter or augment the behaviour of an operating system, of applications, or of other software components by intercepting function calls or messages or events passed between software components. Code that handles such intercepted function calls, events or messages is called a hook.\n",
      "\n",
      "Hooking is used for many purposes, including debugging and extending functionality.\n",
      "Examples might include intercepting keyboard or mouse event messages before they reach an application, or intercepting operating system calls in order to monitor behavior or modify the function of an application or other component. It is also widely used in benchmarking programs, for example frame rate measuring in 3D games, where the output and input is done through hooking.\n",
      "\n",
      "Hooking can also be used by malicious code. For example, rootkits, pieces of software that try to make themselves invisible by faking the output of API calls that would otherwise reveal their existence, often use hooking techniques.\n",
      "\n",
      "Methods \n",
      "Typically hooks are inserted while software is already running, but hooking is a tactic that can also be employed prior to the application being started. Both these techniques are described in greater detail below.\n",
      "\n",
      "Source modification \n",
      "By modifying the source of the executable or library before an application is running, through techniques of reverse engineering, you can also achieve hooking. This is typically used to intercept function calls to either monitor or replace them entirely.\n",
      "\n",
      "For example, by using a disassembler, the entry point of a function within a module can be found. It can then be altered to instead dynamically load some other library module and then have it execute desired methods within that loaded library. If applicable, another related approach by which hooking can be achieved is by altering the import table of an executable. This table can be modified to load any additional library modules as well as changing what external code is invoked when a function is called by the application.\n",
      "Document 3:::\n",
      "The extern keyword applied to a function prototype does absolutely nothing (the extern keyword applied to a function definition is, of course, non-sensical). A function prototype is always a declaration and never a definition. Also, in standard C, a function is always external, but some compiler extensions allow a function to be defined inside a function.\n",
      "\n",
      "Scope, lifetime and the static keyword \n",
      "\n",
      "An external variable can be accessed by all the functions in all the modules of a program. It is a global variable. For a function to be able to use the variable, a declaration or the definition of the external variable must lie before the function definition in the source code. Or there must be a declaration of the variable, with the keyword extern, inside the function.\n",
      "\n",
      "The static keyword (static and extern are mutually exclusive), applied to the definition of an external variable, changes this a bit: the variable can only be accessed by the functions in the same module where it was defined. But it is possible for a function in the same module to pass a reference (pointer) of the variable to another function in another module. In this case, even though the function is in another module, it can read and modify the contents of the variable—it just cannot refer to it by name.\n",
      "\n",
      "It is also possible to use the static keyword on the definition of a local variable. Without the static keyword, the variable is automatically allocated when the function is called and released when the function exits (thus the name \"automatic variable\"). Its value is not retained between function calls. With the static keyword, the variable is allocated when the program starts and released when the program ends. Its value is not lost between function calls. The variable is still local, since it can only be accessed by name inside the function that defined it. But a reference (pointer) to it can be passed to another function, allowing it to read and modify the contents of the variable (again without referring to it by name).\n",
      "\n",
      "External variables are allocated and initialized when the program starts, and the memory is only released when the program ends. Their lifetime is the same as the program's.\n",
      "Document 4:::\n",
      "Tail-recursive functions\n",
      "Tail-recursive functions are functions in which all recursive calls are tail calls and hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive.  In contrast, the factorial function (also below) is not tail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes.  With a compiler or interpreter that treats tail-recursive calls as jumps rather than function calls, a tail-recursive function such as gcd will execute using constant space.  Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops.\n",
      "\n",
      "The significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the call stack; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.\n",
      "\n",
      "Order of execution\n",
      "\n",
      "Consider these two functions:\n",
      "\n",
      "Function 1\n",
      "void recursiveFunction(int num) {\n",
      "    printf(\"%d\\n\", num);\n",
      "    if (num < 4)\n",
      "        recursiveFunction(num + 1);\n",
      "}\n",
      "\n",
      "Function 2\n",
      "void recursiveFunction(int num) {\n",
      "    if (num < 4)\n",
      "        recursiveFunction(num + 1);\n",
      "    printf(\"%d\\n\", num);\n",
      "}\n",
      "\n",
      "Function 2 is function 1 with the lines swapped.\n",
      "\n",
      "In the case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached. \n",
      "\n",
      "Also note that the order of the print statements is reversed, which is due to the way the functions and statements are stored on the call stack.\n",
      "\n",
      "Recursive procedures\". \n",
      "        If the question has options, specify the ID of the correct answer (A, B, C or D).\n",
      "        Think step by step and explain your reasoning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1522 [00:38<2:38:57,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: The Current Privilege Level (CPL) before invoking the syscall function is 3, and after executing the int 0x30 instruction, the CPL is 0. Therefore, the correct option is C. 3 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1522 [00:38<2:41:04,  6.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m questions \u001b[38;5;241m=\u001b[39m full_preference_pairs\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_predictions_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mgenerate_predictions_zero_shot\u001b[0;34m(questions, model_args)\u001b[0m\n\u001b[1;32m     11\u001b[0m chat \u001b[38;5;241m=\u001b[39m Chat\u001b[38;5;241m.\u001b[39mcreate(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# _, context = get_most_relevant_document(question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m _, context \u001b[38;5;241m=\u001b[39m \u001b[43mget_most_relevant_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKNOWLEDGE_VECTOR_DATABASE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# No reranker to have different documents\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(\"The context is:\", context)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m initial_prompt(question, context)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mget_most_relevant_document\u001b[0;34m(question, knowledge_index, reranker, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_most_relevant_document\u001b[39m(\n\u001b[1;32m      2\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      3\u001b[0m     knowledge_index: FAISS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Gather documents with retriever\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(\"=> Retrieving documents...\")\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mknowledge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     relevant_docs \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m relevant_docs]  \u001b[38;5;66;03m# Keep only the text\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Optionally rerank results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:402\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    404\u001b[0m         embedding,\n\u001b[1;32m    405\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:154\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:113\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:94\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[0;32m---> 94\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_multi_process_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mencode_multi_process(texts, pool)\n\u001b[1;32m     96\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:462\u001b[0m, in \u001b[0;36mSentenceTransformer.start_multi_process_pool\u001b[0;34m(self, target_devices)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device_id \u001b[38;5;129;01min\u001b[39;00m target_devices:\n\u001b[1;32m    457\u001b[0m     p \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    458\u001b[0m         target\u001b[38;5;241m=\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39m_encode_multi_process_worker,\n\u001b[1;32m    459\u001b[0m         args\u001b[38;5;241m=\u001b[39m(device_id, \u001b[38;5;28mself\u001b[39m, input_queue, output_queue),\n\u001b[1;32m    460\u001b[0m         daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    461\u001b[0m     )\n\u001b[0;32m--> 462\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_queue, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_queue, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m: processes}\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/modern_nlp/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "questions = full_preference_pairs.to_dict('records')\n",
    "generate_predictions_zero_shot(questions, model_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
